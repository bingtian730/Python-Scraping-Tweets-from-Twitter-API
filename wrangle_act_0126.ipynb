{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open csv file\n",
    "import pandas as pd\n",
    "df_archive=pd.read_csv('twitter-archive-enhanced.csv')\n",
    "df_archive=df_archive.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open tsv file\n",
    "import csv\n",
    "\n",
    "with open('image-predictions.tsv') as tsvfile:\n",
    "  reader = csv.reader(tsvfile, delimiter='\\t')\n",
    "  for row in reader:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_info = pd.read_csv('image-predictions.tsv',delimiter='\\t',nrows=20,encoding='utf-8-sig')#skiprows=1000000, \n",
    "print(list(search_info.columns.values)) #file header\n",
    "print(search_info[[search_info.columns[0],search_info.columns[1],search_info.columns[2]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_info=search_info.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export tweet_id from df_archive as a list\n",
    "tweet_id=df_archive['tweet_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweet_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the tweet_id list to gather data from API\n",
    "import tweepy\n",
    "\n",
    "consumer_key = 'wmrQJf8F8LYiJQLUwb79IDxIT'\n",
    "consumer_secret = 'YnVLTlGZVP9JmSkRkofjzQofWMLu0yYUonlRfjh6yjn5yw33sw'\n",
    "access_token = '1076983945047220224-5NEF8bbuvnmXyjwIkKEdzCljJGNZuj'\n",
    "access_secret = 'dY3jcFFVw8OYTqKeSuCA0KH4pc2po2UGJ8ONsxEdjv78a'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start= time.time()\n",
    "print(\"start\")\n",
    "df_list=[]\n",
    "for id in tweet_id:\n",
    "    try:\n",
    "        tweet = api.get_status(id,tweet_mode='extended')\n",
    "        text=tweet.full_text\n",
    "        retweet_count=tweet.retweet_count\n",
    "        retweeted=tweet.retweet_count\n",
    "        favorite_count=tweet.favorite_count\n",
    "        favorited=tweet.favorited\n",
    "        df_list.append({'id':id, 'full_text':text,'retweet_count':retweet_count,'retweeted':retweeted,\n",
    "                       'favorite_count':favorite_count,'favorited':favorited})\n",
    "        print(df_list)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print('id:'+ 'NA')\n",
    "\n",
    "        end=time.time()\n",
    "        print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the API data into a dataframe\n",
    "df_text=pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe into a csv data file\n",
    "df_text.to_csv('tweet_text.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save API data into a json string file\n",
    "def obj_dict(obj):\n",
    "    return obj.__dict__\n",
    "\n",
    "json_string = json.dumps(df_list, default=obj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the JSON string file into a txt file\n",
    "with open('tweet_json.txt','w') as outfile:\n",
    "    json.dump(json_string,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open and check the txt file\n",
    "with open('tweet_json.txt')as json_file:\n",
    "    data=json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet=pd.read_json(data)\n",
    "df_tweet=df_tweet.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Assessing data & Cleaning data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidiness issue 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define\n",
    "\n",
    "Notice there are columns that are not useful for data analysis, and there're columns only have NaN values. I will clean/delete those columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive.drop(['in_reply_to_status_id','in_reply_to_user_id','retweeted_status_id','retweeted_status_user_id','retweeted_status_timestamp','rating_denominator'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check if there's duplicated rows\n",
    "df_archive.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataframe\n",
    "df_archive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there's any null values \n",
    "df_archive.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality issue 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define \n",
    "Notice the expanded_urls columns have 59 null values, delete this column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_archive.drop(['expanded_urls'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g. Check the statistic for the data frame\n",
    "df_archive.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality issue 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the rating_numerator numbers is within a wide range 10-1776, so I'll check\n",
    "the unique values in this column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive['rating_numerator'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i. Since this columns values runs into a very wide range, categorize these values\n",
    "# into different levels, so that it can be easily analysed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the statisticds , create Bin edges that will be used to \"cut\" the data into groups\n",
    "bin_rating_edges=[0,10,11,12,1780]\n",
    "bin_rating_names=['0-10','11','12','>12']\n",
    "df_archive['rating_levl'] = pd.cut(df_archive['rating_numerator'], bin_rating_edges, labels=bin_rating_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidiness issue 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice there're 4 columns for \"dog stage \" information, make them into one column \"stage\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive['stage'] = 'doggo'\n",
    "df_archive.loc[df_archive['floofer'] == 'floofer', 'stage'] = 'floofer'\n",
    "df_archive.loc[df_archive['pupper'] =='pupper', 'stage'] = 'pupper'\n",
    "df_archive.loc[df_archive['puppo']=='puppo','stage']='puppo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k. Drop the useless columns \n",
    "df_archive.drop(['timestamp','source','text','rating_numerator','name','doggo','floofer','pupper','puppo'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_archive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open json file and show it as a dataframe \n",
    "import json\n",
    "with open('tweet_json.txt')as json_file:\n",
    "    data=json.load(json_file) \n",
    "df_tweet=pd.read_json(data)\n",
    "df_tweet=df_tweet.copy()\n",
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info\n",
    "df_tweet.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null\n",
    "df_tweet.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics\n",
    "df_tweet.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality issue 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After checked the statistic for the data frame, I noticed the favorite_count and retweet_count are within a very wide range of numbers , and they’re not evenly distributed, so I categorize these values into different levels based on the statistics, and created a number categorized data columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the statistic, create category for favorite_count and retweet_count\n",
    "# Bin edges that will be used to \"cut\" the data into groups\n",
    "bin_favorite_edges=[0,5500,11100,20000,141000]\n",
    "bin_favorite_names=['0-5000','5000-11100','11100-20000','20000-141000']\n",
    "bin_retweet_edges=[0,2000,3500,6000,62000]\n",
    "bin_retweet_names=['0-2000','2000-3500','3500-6000','6000-62000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns that categorize retweet_count and favorite_count\n",
    "df_tweet['favor_count'] = pd.cut(df_tweet['favorite_count'], bin_favorite_edges,labels=bin_favorite_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet['retwe_count']=pd.cut(df_tweet['retweet_count'], bin_retweet_edges, labels=bin_retweet_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nulls\n",
    "df_tweet.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality issue 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some null_values in favor_count, drop the null_value rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls \n",
    "df_tweet.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality issue 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some duplicated columns like the retweet_count and retweeted is the same column, dropped retweeted columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicated columns\n",
    "df_tweet.drop(['favorited','retweeted'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality issue 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticed the ‘full_text’ column is not going to be used for analysis, delete the column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.drop(['full_text'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality issue 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to merge the tweet_json and twitter-archive-enhaced data later, I changed the ‘id’ to ‘tweet_id’ to match the same column name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column so that the data frames can merge later \n",
    "df_tweet.rename(index=str,columns={'id':'tweet_id'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes and create a new dataframe df_new\n",
    "frames=[df_tweet,df_archive]\n",
    "df_new=df_tweet.merge(df_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quality issue 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After merged the data, noticed there’re null values , so I removed the null_value rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweet.dropna(axis=0, how='any', thresh=None, subset=None, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into a tidy_master file\n",
    "df_new.to_csv('twitter_archive_master.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.read_csv('twitter_archive_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df_new.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. get average favorite_count for each stage\n",
    "favc_rating = df_new.groupby(['stage']).mean()['favorite_count']\n",
    "favc_rating.sort_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. get average retweet_count for each stage\n",
    "retc_rating = df_new.groupby(['stage']).mean()['retweet_count']\n",
    "retc_rating.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.arange(len(favc_rating))  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c.  plot bars for dog ratings by stage\n",
    "blue_bars = plt.bar(ind, favc_rating.sort_values(), width, color='b', alpha=.7, label='dog rating')\n",
    "# title and labels\n",
    "plt.ylabel('dog favorite ratings')\n",
    "plt.xlabel('dog stage')\n",
    "plt.title('Average Dog Favorite Ratings by Stage')\n",
    "locations = ind + width / 2  # xtick locations\n",
    "labels = ['doggo', 'pupper', 'floofer', 'puppo']  # xtick labels\n",
    "plt.xticks(locations, labels)\n",
    "\n",
    "# legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_bars = plt.bar(ind, retc_rating.sort_values(), width, color='g', alpha=.7, label='favorite count')\n",
    "# title and labels\n",
    "plt.ylabel('Dog retweet counts')\n",
    "plt.xlabel('dog stage')\n",
    "plt.title('Average retweet count by Stage')\n",
    "locations = ind + width / 2  # xtick locations\n",
    "labels = ['doggo', 'pupper', 'floofer','puppo']  # xtick labels\n",
    "plt.xticks(locations, labels)\n",
    "\n",
    "# legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(42)\n",
    "\n",
    "full_data = pd.read_csv('search_info.csv')\n",
    "full_data=full_data.copy()\n",
    "sample_data = full_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the prediction 1 from the data sample\n",
    "sample_mean=full_data['p1_conf'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping from the sample set, select 10 sample data each time and simulate for 10000 times.\n",
    "# And get the mean from the bootstrap \n",
    "means=[]\n",
    "for _ in range(10000):\n",
    "    bootsample= sample_data.sample(10,replace=True)\n",
    "    means.append(bootsample.p1_conf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation from the bootstrapping means\n",
    "np.std(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed mean of prediction one is sample_mean=0.5156\n",
    "# Null hypothesis p1_whole= p1_sample , which the mean of whole prediction 1 is eqalue to the sample data prediction 1. \n",
    "# alpha = 5%\n",
    "# Ho: P1= 0.5156\n",
    "# H1: P1><0.5156\n",
    "null_vals=np.random.normal(0.5156,np.std(means),10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p_1 distribution from the null\n",
    "plt.hist(null_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_vals.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value= (null_vals>sample_mean).mean()+(null_vals<sample_mean).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_value =1, we fail to reject the null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
